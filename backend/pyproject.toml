[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "notebooklm-backend"
version = "0.1.0"
description = "Offline Notebook LM backend focused on local chat via Ollama."
readme = "README.md"
requires-python = ">=3.10"
license = { text = "MIT" }
authors = [
  { name = "Offline Notebook LM Team" }
]
dependencies = [
  "fastapi>=0.115.6,<0.116",
  "uvicorn[standard]>=0.32,<0.33",
  "pydantic>=2.9,<2.10",
  "pydantic-settings>=2.5,<3.0",
  "httpx>=0.27,<0.28",
  "orjson>=3.10,<4.0",
  "chromadb>=0.5,<0.6",
  "sentence-transformers>=2.7,<3.0",
  "python-multipart>=0.0.12",
  "pypdf>=5.0,<6.0",
  "python-docx>=1.1,<2.0",
  "python-pptx>=0.6,<1.0",
  "psutil>=6.1,<7.0",
  # Optional framework integrations (offline-friendly only)
  "langchain-text-splitters>=0.2.0,<0.4.0",
  "llama-index-core>=0.11.0,<0.12.0",
    "llama-index-vector-stores-chroma>=0.2.0,<0.3.0",
    "llama-index-llms-ollama>=0.3.0,<0.4.0",
]

[project.optional-dependencies]
dev = [
  "ruff>=0.9.1,<0.10",
  "pytest>=8.3,<9.0",
  "pytest-asyncio>=0.24,<0.25"
]
gpu = [
  "llama-cpp-python>=0.2.90,<0.3.0",
  "onnxruntime-genai>=0.3.0,<0.5.0",
]
speech = [
  "faster-whisper>=1.0.0,<2.0.0",
  "piper-tts>=1.2.0,<2.0.0",
]

[tool.hatch.metadata]
allow-direct-references = true

[tool.hatch.build.targets.wheel]
packages = ["notebooklm_backend"]

[tool.ruff]
line-length = 120
target-version = "py312"

[tool.ruff.lint]
# Keep CI fast and frictionless in v1: only catch obvious errors (E,F).
select = ["E", "F"]
ignore = []

[tool.pytest.ini_options]
addopts = "-q"
testpaths = ["tests"]

[tool.mypy]
python_version = "3.12"
ignore_missing_imports = true
warn_unused_ignores = true
warn_redundant_casts = true
